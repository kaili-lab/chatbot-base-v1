# KnowledgeChat

![Next.js](https://img.shields.io/badge/Next.js_15-App_Router-black?logo=next.js&logoColor=white)
![TypeScript](https://img.shields.io/badge/TypeScript-5-3178C6?logo=typescript&logoColor=white)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-pgvector-336791?logo=postgresql&logoColor=white)
![Vercel AI SDK](https://img.shields.io/badge/Vercel_AI_SDK-streaming-black?logo=vercel&logoColor=white)
![Better Auth](https://img.shields.io/badge/Better_Auth-email_%2B_OAuth-22c55e)

> Turn your documents into a queryable knowledge base â€” full RAG pipeline with pgvector semantic search, streaming AI responses, and bring-your-own-LLM support.

[ğŸ”— Live Demo](#demo) Â· [ä¸­æ–‡æ–‡æ¡£](./README_CN.md)

---

<!-- SCREENSHOT: Record a 5â€“8s GIF of the core flow: type a question â†’ streaming response appears â†’ "Sources referenced" section expands to show cited documents. Place the file at public/demo.gif and replace this comment with: ![Demo](./public/demo.gif) -->
> **Screenshot pending** â€” GIF will be added after live deployment.

---

## âœ¨ Highlights

**End-to-end RAG pipeline** â€” Upload Markdown or plain-text files; the system automatically parses, chunks (Markdown-aware recursive splitting), and embeds them into pgvector. Every AI response cites the exact source documents with a link back to the original.

**Bring your own LLM** â€” Connect any OpenAI-compatible API (OpenAI, Azure OpenAI, local models via Ollama, etc.). Your API key is AES-encrypted before storage and never sent to the client.

**Honest fallback with source attribution** â€” Similarity threshold dynamically adapts to question length. When no relevant chunks are found, the response is explicitly labeled "general answer" rather than silently hallucinating from an empty context.

---

## Tech Stack

| Layer | Choice |
|---|---|
| Framework | Next.js 15 (App Router) + TypeScript |
| UI | shadcn/ui + Tailwind CSS |
| Database | PostgreSQL + pgvector |
| ORM | Drizzle ORM |
| Auth | Better Auth â€” email/password + Google OAuth |
| AI | Vercel AI SDK + any OpenAI-compatible API |
| Email | Resend |
| Deployment | Vercel |

---

## Features

**Knowledge base management**
- Nested folder tree â€” create folders and sub-folders to organize documents
- Upload `.md` / `.txt` files, or create notes in the built-in Markdown editor (split-pane edit + preview)
- Document detail page â€” view extracted text, file metadata, and processing status

**Document processing pipeline**
- Text extraction â†’ recursive character text splitting (respects Markdown headings, paragraphs, sentences) â†’ vector embedding via `text-embedding-3-small` â†’ stored in pgvector

**Semantic chat**
- Conversation list with star (pinned), rename, and delete
- Streaming responses with full Markdown rendering (headings, code blocks, lists)
- "Sources referenced" collapsible section â€” shows cited document names, click to open original

**LLM configuration**
- Configure base URL, API key, and model per user account
- "Test Connection" validates both the chat and embedding API before saving

**Auth & multi-tenancy**
- Email + password with email verification (Resend), Google OAuth
- All data scoped by `userId` â€” no cross-user data leakage

**UX**
- Dark / light theme toggle
- Collapsible sidebar (icon-only mode)

## Architecture

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚        RAG Pipeline          â”‚
  Upload file â”€â”€â–º  Parse  â”€â”€â–º  Chunk  â”€â”€â–º  Embed  â”€â”€â–º  pgvector
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
  User question â”€â”€â–º  Embed  â”€â”€â–º  cosine similarity â”€â”€â”€â”€â”˜
                                       â”‚
                               Top-K chunks (k=10)
                               + dynamic threshold
                                       â”‚
                              LLM (context injection)
                                       â”‚
                          Streaming response + sources
```

**Key design decisions:**
- Server Components by default; `"use client"` only where interaction is required
- Server Actions for all mutations; Route Handlers only for external-facing APIs
- Chunking uses recursive splitting (500â€“1000 chars, 100-char overlap) â€” no extra model calls needed
- `DocumentParser` interface makes adding PDF/DOCX parsers a drop-in change

## Getting Started

**Prerequisites:** Node.js 18+, pnpm, PostgreSQL with the pgvector extension enabled

1. **Clone and install**
   ```bash
   git clone https://github.com/your-username/knowledgechat.git
   cd knowledgechat
   pnpm install
   ```

2. **Configure environment variables**
   ```bash
   cp .env.local.example .env.local
   ```
   Required variables:

   | Variable | Description |
   |---|---|
   | `DATABASE_URL` | PostgreSQL connection string |
   | `BETTER_AUTH_SECRET` | Random secret (â‰¥ 32 chars) |
   | `BETTER_AUTH_URL` | App base URL (`http://localhost:3000` locally) |
   | `GOOGLE_CLIENT_ID` / `GOOGLE_CLIENT_SECRET` | Google OAuth credentials |
   | `RESEND_API_KEY` / `RESEND_FROM` | Transactional email |
   | `ENCRYPTION_KEY` | AES key for LLM API key encryption |

3. **Run database migrations**
   ```bash
   pnpm db:generate
   pnpm db:migrate
   ```

4. **Start the dev server**
   ```bash
   pnpm dev
   ```
   Open [http://localhost:3000](http://localhost:3000).

5. **Configure your LLM** â€” Go to **Settings**, enter your OpenAI-compatible base URL, API key, and model name, then click **Test Connection**.

## Roadmap

- [ ] PDF and DOCX support â€” `DocumentParser` interface is already defined; implementations pending
- [ ] Live demo deployment

## License

MIT

---

<a name="demo"></a>
> **Demo coming soon.** The live deployment will be linked here once available.
